language,multiple,babel,mbxp,humaneval_x,so_2023_language_percent,so_2023_language_rank,so_tags,github_prs,github_pushes,github_issues,github_stars,stack_gb,codeparrot_gb,alphacode_gb,codegen_gb,polycoder_gb,subreddit_members,subreddit_url,anecdote_1_content,anecdote_1_author,anecdote_1_url,anecdote_2_content,anecdote_2_author,anecdote_2_url,anecdote_3_content,anecdote_3_author,anecdote_3_url
Julia,11,13,N/A,N/A,1.15%,37,"12,402","39,305","166,898","51,276","52,326",3.09,0.29,0,0,0,23.9k,https://reddit.com/r/julia,"I usually start my own articles with ChatGPT but the truth is that right now, if you want to say something interesting in the Julia space, you mostly need to write it yourself since the volume of content about Julia out there isnâ€™t enough for the outputs of ChatGPT to be very useful since our ecosystem is so small.",u/LoganKilpatrick1,https://www.reddit.com/r/Julia/comments/zzvkso/comment/j2i6knx/,"It wasn't trained on sufficient Julia code. As with any machine learning model, ChatGPT is only able to regurgitate what's been fed into it. Also, this behaviour happens with basically every other topic, too. LLMs work by trying to predict what the next word in a sentence would be based on the previous string of words. If a sentence is incomplete, it's going to add a next word. That word is going to be whichever has the highest confidence score, regardless of low that score may actually be. This results in it just making shit up, but often shit that sounds plausible. We've seen CGPT invent academic articles, books, and even entire people because it makes sense to in the sentence it's generating.`",u/Kichae,https://www.reddit.com/r/Julia/comments/112wlle/comment/j8mpgx5/,"I suspect the current language model behind ChatGPT was fed with a lot of code examples from Stack Exchange, but the Julia community mainly uses Discourse instead, which probably wasn't in the training set: https://discourse.julialang.org/",u/Paravalis,https://www.reddit.com/r/Julia/comments/112wlle/comment/j8qzc0j/